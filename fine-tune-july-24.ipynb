{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12577598,"sourceType":"datasetVersion","datasetId":7706209},{"sourceId":486458,"sourceType":"modelInstanceVersion","modelInstanceId":387979,"modelId":406998}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers peft accelerate datasets jsonlines","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:06:28.001999Z","iopub.execute_input":"2025-07-27T17:06:28.002221Z","iopub.status.idle":"2025-07-27T17:07:43.140564Z","shell.execute_reply.started":"2025-07-27T17:06:28.002200Z","shell.execute_reply":"2025-07-27T17:07:43.139668Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sentence-transformers\n!pip install chromadb\n!pip install transformers\n!pip install torch\n!pip install datasets\n!pip install sentence-transformers chromadb transformers torch datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:07:43.142134Z","iopub.execute_input":"2025-07-27T17:07:43.142415Z","iopub.status.idle":"2025-07-27T17:08:30.363252Z","shell.execute_reply.started":"2025-07-27T17:07:43.142385Z","shell.execute_reply":"2025-07-27T17:08:30.362513Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile finetune_tinylama_dop.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nimport jsonlines\nimport os\n\n# --- Configuration ---\nBASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nDATASET_PATH = \"/kaggle/input/dop-dataset/Dataset/combined_dataset.jsonl\"\nOUTPUT_ADAPTERS_DIR = \"./fine_tuned_tinylama_dop_adapters\"\n\n# --- ENVIRONMENT CONFIGURATIONS ---\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n# IMPORTANT: Remove or comment out CUDA_VISIBLE_DEVICES for multi-GPU\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Determine device\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\n    print(f\"CUDA is available. Training will use GPU(s). Total GPUs detected: {torch.cuda.device_count()}\")\nelse:\n    DEVICE = \"cpu\"\n    print(\"WARNING: CUDA is not available. Training will be done on CPU, which will be very slow.\")\n\n# --- 1. Load and Prepare Dataset ---\ndef load_and_prepare_dataset(tokenizer_obj: AutoTokenizer, dataset_file: str):\n    \"\"\"Loads a single .jsonl file, formats, tokenizes, and splits it.\"\"\"\n    raw_data = []\n    with jsonlines.open(dataset_file) as reader:\n        for obj in reader:\n            raw_data.append(obj)\n\n    formatted_texts = []\n    for item in raw_data:\n        instruction = item[\"instruction\"]\n        output = item[\"output\"]\n        formatted_texts.append(f\"<s>[INST] {instruction} [/INST] {output}</s>\")\n\n    # Create a single dataset first\n    full_dataset = Dataset.from_dict({\"text\": formatted_texts})\n\n    # Tokenize the entire dataset\n    def tokenize_function(examples):\n        tokenized_output = tokenizer_obj(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=512,\n        )\n        # Standard causal LM - labels are the same as input_ids\n        tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n        return tokenized_output\n\n    tokenized_dataset = full_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n    \n    # Split the dataset into training and validation sets (e.g., 90% train, 10% validation)\n    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n\n    print(f\"Dataset loaded from {dataset_file}\")\n    print(f\"Training set size: {len(split_dataset['train'])}\")\n    print(f\"Validation set size: {len(split_dataset['test'])}\")\n    \n    # Return the split datasets\n    return split_dataset['train'], split_dataset['test']\n\n# --- 2. Load Tokenizer and Base Model ---\nprint(f\"Loading tokenizer for {BASE_MODEL_ID}...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    print(\"Added [PAD] token to tokenizer.\")\n\nprint(f\"Loading base model {BASE_MODEL_ID}...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    torch_dtype=torch.bfloat16,\n    # device_map not set here; accelerate handles it for multi-GPU\n)\n\nif tokenizer.pad_token_id is not None:\n    model.resize_token_embeddings(len(tokenizer))\n    # Optional: Initialize the new pad token embedding\n    with torch.no_grad():\n        model.model.embed_tokens.weight.data[tokenizer.pad_token_id] = model.model.embed_tokens.weight.data[tokenizer.eos_token_id]\n\n# --- 3. Configure LoRA ---\nprint(\"Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nif DEVICE == \"cuda\":\n    model.enable_input_require_grads()\n    model.gradient_checkpointing_enable()\n    model.config.use_cache = False\n    print(\"Gradient checkpointing enabled and use_cache set to False. Input gradients enabled.\")\n\nmodel.print_trainable_parameters()\n\n# --- 4. Load and Prepare Training & Validation Datasets ---\ntrain_dataset, eval_dataset = load_and_prepare_dataset(tokenizer, DATASET_PATH)\n\n# --- 5. Define Training Arguments ---\nprint(\"Defining training arguments for optimal multi-GPU training...\")\nuse_bf16 = False\nuse_fp16 = False\nif DEVICE == \"cuda\":\n    if torch.cuda.get_device_properties(0).major >= 8:  # Ampere or newer (RTX 30xx, A100, H100)\n        use_bf16 = True\n        print(\"Using bfloat16 (BF16) precision for training.\")\n    else:  # Older GPUs like T4, P100 (Volta, Pascal, etc.)\n        use_fp16 = True\n        print(\"Using float16 (FP16) precision for training.\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_ADAPTERS_DIR,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    num_train_epochs=4,\n    logging_dir=\"./logs_tinylama_dop\",\n    logging_steps=10,\n    save_steps=200,\n    save_total_limit=2,\n    report_to=\"tensorboard\",\n    bf16=use_bf16,\n    fp16=use_fp16,\n    overwrite_output_dir=True,\n    gradient_checkpointing=True if DEVICE == \"cuda\" else False,\n    ddp_find_unused_parameters=False,\n    \n    # --- Arguments for Validation ---\n    eval_strategy=\"steps\",\n    eval_steps=50,  # Evaluate every 50 steps\n    per_device_eval_batch_size=4,\n)\n\n# --- 6. Create Trainer and Start Training ---\nprint(\"Initializing Trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,  # Pass in the validation dataset\n    tokenizer=tokenizer,\n)\n\nprint(\"\\nStarting fine-tuning process...\")\ntrainer.train()\nprint(\"\\nFine-tuning complete. LoRA adapters saved to:\", OUTPUT_ADAPTERS_DIR)\n\n# --- End of Part 1 ---\nprint(\"\\n-----------------------------------------------------\")\nprint(\"Fine-tuning (Part 1) complete. LoRA adapters saved.\")\nprint(\"Proceed to Part 2 to merge adapters and save the full model.\")\nprint(\"-----------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:13.113891Z","iopub.execute_input":"2025-07-27T17:16:13.114593Z","iopub.status.idle":"2025-07-27T17:16:13.122608Z","shell.execute_reply.started":"2025-07-27T17:16:13.114560Z","shell.execute_reply":"2025-07-27T17:16:13.121905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch finetune_tinylama_dop.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:18.102855Z","iopub.execute_input":"2025-07-27T17:16:18.103131Z","iopub.status.idle":"2025-07-27T17:39:54.818796Z","shell.execute_reply.started":"2025-07-27T17:16:18.103108Z","shell.execute_reply":"2025-07-27T17:39:54.818019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, set_peft_model_state_dict, PeftModel\nimport os\n# --- ADDED IMPORT ---\nimport safetensors.torch # Import the safetensors library\n# --- END ADDED IMPORT ---\n\n# --- Configuration (Must match Part 1's configuration) ---\nBASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# Directory where LoRA adapters were saved by Part 1\nOUTPUT_ADAPTERS_DIR = \"./fine_tuned_tinylama_dop_adapters/checkpoint-80\" # Confirmed correct path\n# Directory where the final merged fine-tuned model will be saved\nOUTPUT_MERGED_MODEL_DIR = \"./merged_fine_tuned_tinylama_dop\"\n\n# --- Determine device for loading models ---\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\n    print(f\"CUDA is available. Merging will use GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    DEVICE = \"cpu\"\n    print(\"WARNING: CUDA is not available. Merging will be done on CPU.\")\n\n# --- 1. Load Tokenizer and Base Model ---\nprint(f\"Loading tokenizer for {BASE_MODEL_ID}...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\nprint(f\"Loading base model {BASE_MODEL_ID} for merging...\")\nbase_model_for_merge = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n)\n\n# --- 2. Load LoRA Adapters and Merge (Manual Load) ---\nprint(f\"Loading LoRA adapters weights from: {os.path.abspath(OUTPUT_ADAPTERS_DIR)}\")\n\nlora_config = LoraConfig.from_pretrained(os.path.abspath(OUTPUT_ADAPTERS_DIR))\n\nmerged_model = get_peft_model(base_model_for_merge, lora_config)\n\nadapter_weights_path = os.path.join(os.path.abspath(OUTPUT_ADAPTERS_DIR), \"adapter_model.bin\")\nif not os.path.exists(adapter_weights_path):\n    adapter_weights_path = os.path.join(os.path.abspath(OUTPUT_ADAPTERS_DIR), \"adapter_model.safetensors\")\n    if not os.path.exists(adapter_weights_path):\n        raise FileNotFoundError(f\"Neither adapter_model.bin nor adapter_model.safetensors found in {os.path.abspath(OUTPUT_ADAPTERS_DIR)}\")\n\nprint(f\"Loading adapter weights from: {adapter_weights_path}\")\n# CRITICAL FIX: Use safetensors.torch.load_file for .safetensors files\nadapter_state_dict = safetensors.torch.load_file(adapter_weights_path, device=\"cpu\") # map_location=\"cpu\" is replaced by device=\"cpu\"\n\nset_peft_model_state_dict(merged_model, adapter_state_dict)\n\nprint(\"Merging LoRA adapters into the base model...\")\nmerged_model = merged_model.merge_and_unload()\n\n# --- 3. Save the Merged Fine-Tuned Model ---\nprint(f\"Saving the full fine-tuned model to: {OUTPUT_MERGED_MODEL_DIR}\")\nos.makedirs(OUTPUT_MERGED_MODEL_DIR, exist_ok=True)\nmerged_model.save_pretrained(OUTPUT_MERGED_MODEL_DIR)\ntokenizer.save_pretrained(OUTPUT_MERGED_MODEL_DIR)\n\nprint(\"\\n--- Merging Complete ---\")\nprint(\"Your full fine-tuned model (base model + LoRA adapters) has been saved.\")\nprint(f\"Model path: {os.path.abspath(OUTPUT_MERGED_MODEL_DIR)}\")\nprint(\"\\n--- Next Steps ---\")\nprint(\"1. **Evaluate/Test the Model:** Load the merged model and evaluate its performance.\")\nprint(\"2. **Update Chatbot:** Update your `NEEPCOPolicyChatbot` code to use this fine-tuned model.\")\nprint(\"3. **(Optional) Quantize the Model:** If you need a lighter model for inference, proceed with quantization.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:41:23.542877Z","iopub.execute_input":"2025-07-27T17:41:23.543570Z","iopub.status.idle":"2025-07-27T17:41:30.905924Z","shell.execute_reply.started":"2025-07-27T17:41:23.543542Z","shell.execute_reply":"2025-07-27T17:41:30.904982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, set_peft_model_state_dict, PeftModel\nimport os\nimport safetensors.torch  # Import safetensors\n\n# === Configurations (must match Part 1) ===\nBASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nOUTPUT_ADAPTERS_DIR = \"./fine_tuned_tinylama_dop_adapters/checkpoint-80\"\nOUTPUT_MERGED_MODEL_DIR = \"/kaggle/working/output/merged_tinylama_dop\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n    device_map=\"auto\" if device==\"cuda\" else None,\n)\n\n# Load LoRA\nlora_config = LoraConfig.from_pretrained(OUTPUT_ADAPTERS_DIR)\nmodel = get_peft_model(base_model, lora_config)\n\n# Load adapter weights\nadapter_path = os.path.join(OUTPUT_ADAPTERS_DIR, \"adapter_model.bin\")\nif not os.path.exists(adapter_path):\n    adapter_path = os.path.join(OUTPUT_ADAPTERS_DIR, \"adapter_model.safetensors\")\n    if not os.path.exists(adapter_path):\n        raise FileNotFoundError(\"adapter_model.bin or .safetensors not found\")\n\nstate = safetensors.torch.load_file(adapter_path, device=\"cpu\")\nset_peft_model_state_dict(model, state)\n\n# Merge LoRA\nmodel = model.merge_and_unload()\n\n# Save merged model into notebook output path\nprint(f\"Saving merged model to {OUTPUT_MERGED_MODEL_DIR}\")\nos.makedirs(OUTPUT_MERGED_MODEL_DIR, exist_ok=True)\nmodel.save_pretrained(OUTPUT_MERGED_MODEL_DIR)\ntokenizer.save_pretrained(OUTPUT_MERGED_MODEL_DIR)\nprint(\"✅ Saved merged model and tokenizer to notebook output directory.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:45:25.046354Z","iopub.execute_input":"2025-07-27T17:45:25.047171Z","iopub.status.idle":"2025-07-27T17:45:32.464666Z","shell.execute_reply.started":"2025-07-27T17:45:25.047146Z","shell.execute_reply":"2025-07-27T17:45:32.463984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom typing import List, Dict, Any\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport os\n\ndef create_optimized_chunks(context_file_path: str) -> List[Dict]:\n    \"\"\"\n    Converts combined_context.jsonl into hyper-granular, context-rich chunks\n    using an advanced strategy for optimal RAG performance.\n    \"\"\"\n    chunks = []\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=700,\n        chunk_overlap=100,\n        length_function=len,\n        add_start_index=False,\n    )\n\n    with open(context_file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            data = json.loads(line.strip())\n\n            section = data.get('section', '')\n            title = data.get('title', '')\n            main_clause_num = data.get('clause') or data.get('Clause', '')\n\n            if 'subclauses' in data:\n                for subclause in data['subclauses']:\n                    if 'methods' in subclause:\n                        for method in subclause['methods']:\n                            _process_and_append_chunk(\n                                text_splitter, chunks, method,\n                                parent_context={\n                                    'section': section, 'title': title, 'main_clause_num': main_clause_num,\n                                    'subclause_id': subclause.get('id', ''), 'subclause_desc': subclause.get('description', ''),\n                                    'subclause_remarks': subclause.get('remarks', [])\n                                }\n                            )\n                    else:\n                        _process_and_append_chunk(text_splitter, chunks, subclause, {'section': section, 'title': title, 'main_clause_num': main_clause_num})\n            else:\n                _process_and_append_chunk(text_splitter, chunks, data, {'section': section, 'title': title, 'main_clause_num': main_clause_num})\n    return chunks\n\ndef _process_and_append_chunk(text_splitter: RecursiveCharacterTextSplitter, chunks: List[Dict], content_dict: Dict[str, Any], parent_context: Dict[str, Any]):\n    \"\"\"Helper function to construct, split, and append a context-aware chunk.\"\"\"\n    section, title, main_clause_num = parent_context.get('section'), parent_context.get('title'), parent_context.get('main_clause_num')\n    sub_title = content_dict.get('title', '')\n\n    is_method_level = 'method' in content_dict\n    if is_method_level:\n        sub_id = parent_context.get('subclause_id', '')\n        method_id = content_dict.get('id', '')\n        method_type = content_dict.get('method', '')\n    else:\n        sub_id = content_dict.get('id', '')\n        method_id = ''\n        method_type = ''\n\n    full_clause_id = f\"{main_clause_num or ''}{sub_id or ''}{method_id or ''}\"\n\n    chunk_text = \"\"\n    if section == 'Annexure A':\n        chunk_text += \"This item is a matter requiring approval of the Board of Directors.\\n\"\n\n    chunk_text += f\"Section {section}: {title}\\nClause {full_clause_id}: \"\n    if sub_title: chunk_text += f\"{sub_title} - \"\n    if method_type: chunk_text += f\"{method_type} - \"\n\n    description = content_dict.get('description', parent_context.get('subclause_desc', ''))\n    if description: chunk_text += f\"{description}\\n\"\n\n    items = content_dict.get('items', [])\n    if items: chunk_text += f\"Items: {'; '.join(str(item) for item in items)}\\n\"\n\n    delegation = content_dict.get('delegation')\n    if delegation:\n        delegation_parts = []\n        if isinstance(delegation, dict):\n            for grade, power in delegation.items():\n                if power and str(power).strip() not in [\"NIL\", \"---\"]:\n                    delegation_parts.append(f\"{grade}: {power}\")\n        elif isinstance(delegation, str) and delegation.strip() not in [\"NIL\", \"---\"]:\n            delegation_parts.append(delegation)\n        if delegation_parts:\n            chunk_text += \"Delegation - \" + '; '.join(delegation_parts) + \"\\n\"\n\n    remarks = content_dict.get('remarks', parent_context.get('subclause_remarks', []))\n    if remarks:\n        if isinstance(remarks, str) and 'remarks_reference' in content_dict:\n             chunk_text += f\"Remarks Reference: {content_dict['remarks_reference']}\\n\"\n        elif isinstance(remarks, list):\n             chunk_text += f\"Remarks: {' '.join(str(r) for r in remarks)}\\n\"\n\n    base_chunk_text = chunk_text.strip()\n    if not base_chunk_text: return\n\n    split_texts = text_splitter.split_text(base_chunk_text)\n    for i, split_text in enumerate(split_texts):\n        chunk_id = f\"sec_{section}_cl_{full_clause_id}_part_{i}\".replace(\" \", \"_\").lower()\n        chunks.append({\n            'text': split_text,\n            'metadata': {\n                'section': section, 'title': title, 'clause': str(full_clause_id),\n                'subclause_title': sub_title or 'N/A', 'method': method_type or 'N/A'\n            },\n            'id': chunk_id\n        })\n\nif __name__ == \"__main__\":\n    INPUT_CONTEXT_PATH = \"/kaggle/input/dop-dataset/Context/combined_context.jsonl\"\n    OUTPUT_CHUNKS_PATH = \"processed_chunks_final.json\"\n\n    if not os.path.exists(INPUT_CONTEXT_PATH):\n        print(f\"FATAL ERROR: The context file was not found at '{INPUT_CONTEXT_PATH}'\")\n    else:\n        try:\n            print(f\"Starting chunk creation from '{INPUT_CONTEXT_PATH}'...\")\n            final_chunks = create_optimized_chunks(INPUT_CONTEXT_PATH)\n\n            with open(OUTPUT_CHUNKS_PATH, \"w\", encoding='utf-8') as f:\n                json.dump(final_chunks, f, indent=2, ensure_ascii=False)\n\n            print(f\"\\n✅ Successfully created {len(final_chunks)} context-aware chunks.\")\n            print(f\"   Output saved to '{OUTPUT_CHUNKS_PATH}'\")\n\n        except Exception as e:\n            print(f\"An error occurred during chunk creation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:20:25.494196Z","iopub.execute_input":"2025-07-26T09:20:25.494453Z","iopub.status.idle":"2025-07-26T09:20:25.520292Z","shell.execute_reply.started":"2025-07-26T09:20:25.494434Z","shell.execute_reply":"2025-07-26T09:20:25.519610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile create_vector_database.py\nimport json\nimport os\nimport shutil\nfrom typing import List, Dict\n\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\n\nclass PolicyVectorDB:\n    \"\"\"Manages the creation and searching of a persistent vector database.\"\"\"\n    def __init__(self, persist_directory: str = \"chroma_db\"):\n        self.client = chromadb.PersistentClient(path=persist_directory)\n        self.collection_name = \"neepco_dop_policies\"\n        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5', device = 'cpu')\n        self.collection = self.client.get_or_create_collection(\n            name=self.collection_name,\n            metadata={\"description\": \"NEEPCO Delegation of Powers Policy\"}\n        )\n        print(f\"Loaded/Created persistent collection '{self.collection_name}' at '{persist_directory}'\")\n\n    def _flatten_metadata(self, metadata: Dict) -> Dict:\n        \"\"\"Ensures all metadata values are strings for ChromaDB compatibility.\"\"\"\n        return {key: str(value) for key, value in metadata.items()}\n\n    def add_chunks(self, chunks: List[Dict]):\n        \"\"\"Encodes and adds a list of chunk dictionaries to the database.\"\"\"\n        if not chunks:\n            print(\"No chunks provided to add.\")\n            return\n\n        existing_ids = set(self.collection.get(include=[])['ids'])\n        new_chunks = [chunk for chunk in chunks if chunk.get('id') not in existing_ids]\n\n        if not new_chunks:\n            print(\"No new chunks to add. All provided chunks already exist in the database.\")\n            return\n\n        print(f\"Found {len(new_chunks)} new chunks to add.\")\n        batch_size = 128\n\n        for i in range(0, len(new_chunks), batch_size):\n            batch = new_chunks[i:i + batch_size]\n            print(f\"  - Processing batch {i//batch_size + 1}/{ -(-len(new_chunks) // batch_size) }...\")\n\n            texts = [chunk['text'] for chunk in batch]\n            ids = [chunk['id'] for chunk in batch]\n            metadatas = [self._flatten_metadata(chunk['metadata']) for chunk in batch]\n\n            embeddings = self.embedding_model.encode(texts, show_progress_bar=False).tolist()\n            self.collection.add(ids=ids, embeddings=embeddings, documents=texts, metadatas=metadatas)\n        \n        print(f\"Successfully added {len(new_chunks)} new chunks to the database!\")\n\n    def search(self, query_text: str, top_k: int = 3) -> List[Dict]:\n        \"\"\"Searches the collection for a given query text.\"\"\"\n        query_embedding = self.embedding_model.encode([query_text]).tolist()\n        results = self.collection.query(\n            query_embeddings=query_embedding,\n            n_results=top_k,\n            include=['documents', 'metadatas', 'distances']\n        )\n        \n        search_results = []\n        if not results.get('documents'):\n            return []\n\n        for i, doc in enumerate(results['documents'][0]):\n            relevance_score = 1 - results['distances'][0][i]\n            search_results.append({\n                'text': doc,\n                'metadata': results['metadatas'][0][i],\n                'relevance_score': relevance_score\n            })\n        return search_results\n\ndef main():\n    \"\"\"Main function to build and verify the vector database.\"\"\"\n    INPUT_CHUNKS_PATH = \"processed_chunks_final.json\"\n    PERSIST_DIRECTORY = \"policy_vector_db\"\n\n    if not os.path.exists(INPUT_CHUNKS_PATH):\n        print(f\"FATAL ERROR: The input chunk file was not found at '{INPUT_CHUNKS_PATH}'\")\n        print(\"Please run 'create_chunks.py' first.\")\n        return\n\n    if os.path.exists(PERSIST_DIRECTORY):\n        print(f\"Removing existing database at '{PERSIST_DIRECTORY}' to ensure a clean build.\")\n        shutil.rmtree(PERSIST_DIRECTORY)\n        \n    print(f\"Creating database directory: '{PERSIST_DIRECTORY}'\")\n    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n    os.chmod(PERSIST_DIRECTORY, 0o777)\n\n    print(\"\\nStep 1: Loading processed chunks...\")\n    with open(INPUT_CHUNKS_PATH, 'r', encoding='utf-8') as f:\n        chunks_to_add = json.load(f)\n    print(f\"Loaded {len(chunks_to_add)} chunks.\")\n    \n    print(\"\\nStep 2: Setting up persistent vector database...\")\n    db = PolicyVectorDB(persist_directory=PERSIST_DIRECTORY)\n    \n    print(\"\\nStep 3: Adding chunks to the database...\")\n    db.add_chunks(chunks_to_add)\n    \n    print(f\"\\n✅ Vector database setup complete. Total chunks in DB: {db.collection.count()}\")\n    print(f\"Database is saved in: {os.path.abspath(PERSIST_DIRECTORY)}\")\n\n    print(\"\\n--- Running Verification Tests ---\")\n    test_questions = [\n        \"Who can approve changes to the pay structure?\",\n        \"What is the financial limit for a DGM for works on a limited tender basis?\",\n        \"What's the delegation power of an ED for single tender O&M contracts from an OEM?\"\n    ]\n\n    for question in test_questions:\n        print(f\"\\n--- Testing Query ---\")\n        print(f\"Query: {question}\")\n        search_results = db.search(question, top_k=2)\n        if search_results:\n            for j, result in enumerate(search_results, 1):\n                print(f\"  Result {j} (Relevance: {result['relevance_score']:.4f}):\")\n                print(f\"  Text: {result['text'][:300]}...\")\n                print(f\"  Metadata: {result['metadata']}\")\n        else:\n            print(\"  No results found.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:20:56.904061Z","iopub.execute_input":"2025-07-26T09:20:56.904558Z","iopub.status.idle":"2025-07-26T09:20:56.911743Z","shell.execute_reply.started":"2025-07-26T09:20:56.904537Z","shell.execute_reply":"2025-07-26T09:20:56.910978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/create_vector_database.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:21:15.158604Z","iopub.execute_input":"2025-07-26T09:21:15.158876Z","iopub.status.idle":"2025-07-26T09:21:34.066622Z","shell.execute_reply.started":"2025-07-26T09:21:15.158856Z","shell.execute_reply":"2025-07-26T09:21:34.065862Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/chroma_db\nprint(\"Cleaned up old ChromaDB folder (if any).\")\n!rm /kaggle/working/create_vector_database.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:19:26.793480Z","iopub.execute_input":"2025-07-26T09:19:26.794087Z","iopub.status.idle":"2025-07-26T09:19:27.184597Z","shell.execute_reply.started":"2025-07-26T09:19:26.794063Z","shell.execute_reply":"2025-07-26T09:19:27.183681Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nimport os\nimport json\n\n# --- Configuration ---\nMERGED_MODEL_PATH = \"/kaggle/working/merged_fine_tuned_tinylama_dop\"\nVECTOR_DB_PATH = \"/kaggle/working/policy_vector_db\"\n\n# --- Class Definition 1: The Correct Vector DB Class ---\nclass PolicyVectorDB:\n    \"\"\"Manages the persistent vector database using ChromaDB.\"\"\"\n    def __init__(self, persist_directory: str):\n        self.client = chromadb.PersistentClient(path=persist_directory)\n        self.collection_name = \"neepco_dop_policies\"\n        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n        self.collection = self.client.get_or_create_collection(name=self.collection_name)\n        print(f\"Loaded/Created persistent collection '{self.collection_name}' at '{persist_directory}'\")\n\n    def search(self, query: str, top_k: int = 3) -> list:\n        \"\"\"Searches the database for a given query.\"\"\"\n        query_embedding = self.embedding_model.encode([query]).tolist()\n        results = self.collection.query(\n            query_embeddings=query_embedding,\n            n_results=top_k,\n            include=['documents', 'metadatas', 'distances']\n        )\n        search_results = []\n        if not results or not results.get('documents') or not results['documents'][0]:\n            return []\n        for i in range(len(results['documents'][0])):\n            relevance_score = 1 - results['distances'][0][i]\n            search_results.append({\n                'text': results['documents'][0][i],\n                'metadata': results['metadatas'][0][i],\n                'relevance_score': relevance_score\n            })\n        return search_results\n\n# --- Class Definition 2: The Interactive Chatbot Class ---\nclass InteractiveChatbot:\n    \"\"\"Handles the full RAG pipeline for interactive chat.\"\"\"\n    def __init__(self, model_path: str, vector_db_path: str):\n        print(f\"Loading fine-tuned model from: {model_path}...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        print(\"Connecting to existing vector database...\")\n        self.vector_db = PolicyVectorDB(vector_db_path)\n        \n        print(\"\\nInteractive Chatbot ready!\")\n\n    def retrieve_context(self, query: str, top_k: int = 3) -> list:\n        \"\"\"Retrieves and filters context from the vector database.\"\"\"\n        retrieved_results = self.vector_db.search(query, top_k=top_k)\n        RELEVANCE_THRESHOLD = 0.1 \n        return [r for r in retrieved_results if r.get('relevance_score', 0) >= RELEVANCE_THRESHOLD]\n    \n    def format_prompt(self, query: str, context_results: list) -> str:\n        \"\"\"Formats the prompt with concise context for the LLM.\"\"\"\n        context_text = \"\"\n        if context_results:\n            for i, result in enumerate(context_results, 1):\n                metadata = result.get(\"metadata\", {})\n                source_info = f\"[Section: {metadata.get('section', 'N/A')}, Clause: {metadata.get('clause', 'N/A')}]\"\n                context_text += f\"Source {i}: {source_info}\\nDetails: {result.get('text', '')}\\n\\n\"\n        else:\n            context_text = \"No specific policy information was found for this question.\"\n            \n        return f\"\"\"<s>[INST] You are a helpful assistant for NEEPCO's Delegation of Power (DOP) policies. Use only the provided policy information to answer questions accurately and completely. Ensure you state the full rule, including all required actions, conditions, and approvals mentioned in the policy. If the provided information is insufficient, state that you cannot answer based on the given policies.\n\nPolicy Information:\n{context_text.strip()}\n\nQuestion: {query} [/INST]\n\nAnswer: Based on the provided policy information,\"\"\"\n\n    def chat(self, query: str):\n        \"\"\"Orchestrates the RAG pipeline and streams the response.\"\"\"\n        context_results = self.retrieve_context(query)\n        \n        if not context_results:\n            print(\"\\nAnswer: I apologize, but I cannot find relevant information in the provided NEEPCO policy documents to answer your question.\")\n            return\n\n        prompt = self.format_prompt(query, context_results)\n        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        print(\"\\nAnswer: \", end=\"\")\n        _ = self.model.generate(\n            **inputs,\n            streamer=streamer,\n            max_new_tokens=512,\n            do_sample=True,\n            temperature=0.1,\n            top_p=0.9,\n            pad_token_id=self.tokenizer.eos_token_id\n        )\n        print(\"\\n\\n---\")\n        print(\"Sources Used:\")\n        for i, r in enumerate(context_results, 1):\n            meta = r['metadata']\n            print(f\"{i}. [Relevance: {r['relevance_score']:.2f}] Section {meta.get('section', 'N/A')}, Clause {meta.get('clause', 'N/A')}\")\n        print(\"-\" * 50)\n\n\n# --- Main execution block ---\ndef start_chat():\n    # Check if required files/folders exist\n    if not os.path.exists(VECTOR_DB_PATH) or not os.path.exists(MERGED_MODEL_PATH):\n        print(\"Error: Database or Model not found.\")\n        print(f\"Please ensure '{VECTOR_DB_PATH}' folder exists by running 'create_vector_database.py' first.\")\n        print(f\"And ensure the fine-tuned model exists at '{MERGED_MODEL_PATH}'.\")\n        return\n\n    chatbot = InteractiveChatbot(model_path=MERGED_MODEL_PATH, vector_db_path=VECTOR_DB_PATH)\n    \n    print(\"\\nType your questions about NEEPCO DOP policies. Type 'quit' to exit.\")\n    while True:\n        try:\n            user_query = input(\"\\n\\nYour question: \").strip()\n            if user_query.lower() == 'quit':\n                print(\"Exiting chat. Goodbye!\")\n                break\n            if user_query:\n                chatbot.chat(user_query)\n        except (KeyboardInterrupt, EOFError):\n            print(\"\\nExiting chat. Goodbye!\")\n            break\n\n# Start the chatbot\nstart_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:31:04.839792Z","iopub.execute_input":"2025-07-26T09:31:04.840375Z","iopub.status.idle":"2025-07-26T10:00:15.828208Z","shell.execute_reply.started":"2025-07-26T09:31:04.840349Z","shell.execute_reply":"2025-07-26T10:00:15.827406Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile quantize_model.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport os\n\n# --- Configuration ---\n# Path to the full-precision merged model from the previous step\nMERGED_MODEL_PATH = \"./merged_fine_tuned_tinylama_dop\"\n# Path where the new, quantized model will be saved\nQUANTIZED_MODEL_PATH = \"./quantized_fine_tuned_tinylama_dop\"\n\n# --- Main Quantization Logic ---\ndef quantize_and_save_model():\n    \"\"\"Loads a model, quantizes it to 4-bit, and saves it.\"\"\"\n\n    print(f\"Loading model from: {MERGED_MODEL_PATH}\")\n\n    # --- 1. Define Quantization Configuration ---\n    # This configures the model to be loaded in 4-bit precision.\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\", # Use NF4 (NormalFloat-4) quantization\n        bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16 for performance\n        bnb_4bit_use_double_quant=True, # Use a second quantization for more memory savings\n    )\n\n    # --- 2. Load the Model with Quantization ---\n    # The `quantization_config` argument applies the 4-bit conversion on-the-fly.\n    model = AutoModelForCausalLM.from_pretrained(\n        MERGED_MODEL_PATH,\n        quantization_config=quantization_config,\n        device_map=\"auto\", # Automatically map model layers to available devices (GPU/CPU)\n    )\n\n    # Also load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n    \n    print(f\"\\nModel loaded and quantized successfully.\")\n    print(\"Model memory footprint:\")\n    print(model.get_memory_footprint())\n\n    # --- 3. Save the Quantized Model ---\n    # The `save_pretrained` method for a quantized model saves the model with its\n    # quantization configuration, making it easy to load later.\n    print(f\"\\nSaving quantized model to: {QUANTIZED_MODEL_PATH}\")\n    os.makedirs(QUANTIZED_MODEL_PATH, exist_ok=True)\n    \n    model.save_pretrained(QUANTIZED_MODEL_PATH)\n    tokenizer.save_pretrained(QUANTIZED_MODEL_PATH)\n    \n    print(\"\\n--- Quantization Complete ---\")\n    print(f\"The 4-bit quantized model is saved at: {os.path.abspath(QUANTIZED_MODEL_PATH)}\")\n    print(\"You can now use this path in your inference script.\")\n\n# --- Start the process ---\nif __name__ == \"__main__\":\n    if not os.path.exists(MERGED_MODEL_PATH):\n        print(f\"Error: Merged model not found at '{MERGED_MODEL_PATH}'.\")\n        print(\"Please ensure you have run the merging script first.\")\n    else:\n        quantize_and_save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:04:39.411769Z","iopub.execute_input":"2025-07-26T10:04:39.412066Z","iopub.status.idle":"2025-07-26T10:04:39.418246Z","shell.execute_reply.started":"2025-07-26T10:04:39.412042Z","shell.execute_reply":"2025-07-26T10:04:39.417363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python quantize_model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:04:53.728528Z","iopub.execute_input":"2025-07-26T10:04:53.729143Z","iopub.status.idle":"2025-07-26T10:05:07.040440Z","shell.execute_reply.started":"2025-07-26T10:04:53.729118Z","shell.execute_reply":"2025-07-26T10:05:07.039504Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile rag_chatbot_quantized.py\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\nimport chromadb\nfrom sentence_transformers import SentenceTransformer\nimport os\n\n# --- Configuration ---\nMODEL_PATH = \"./quantized_fine_tuned_tinylama_dop\"\nVECTOR_DB_PATH = \"/kaggle/working/policy_vector_db\"\n\n# --- Class Definition 1: The Vector DB Class (With Fix) ---\nclass PolicyVectorDB:\n    \"\"\"Manages the persistent vector database using ChromaDB.\"\"\"\n    def __init__(self, persist_directory: str):\n        self.client = chromadb.PersistentClient(path=persist_directory)\n        self.collection_name = \"neepco_dop_policies\"\n        \n        # *** FIX APPLIED HERE: Load embedding model on the CPU ***\n        print(\"Loading embedding model onto CPU...\")\n        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5', device='cpu')\n        \n        self.collection = self.client.get_or_create_collection(name=self.collection_name)\n        print(f\"Loaded/Created persistent collection '{self.collection_name}' at '{persist_directory}'\")\n\n    def search(self, query: str, top_k: int = 3) -> list:\n        \"\"\"Searches the database for a given query.\"\"\"\n        query_embedding = self.embedding_model.encode([query]).tolist()\n        results = self.collection.query(\n            query_embeddings=query_embedding,\n            n_results=top_k,\n            include=['documents', 'metadatas', 'distances']\n        )\n        search_results = []\n        if not results or not results.get('documents') or not results['documents'][0]:\n            return []\n        for i in range(len(results['documents'][0])):\n            relevance_score = 1 - results['distances'][0][i]\n            search_results.append({\n                'text': results['documents'][0][i],\n                'metadata': results['metadatas'][0][i],\n                'relevance_score': relevance_score\n            })\n        return search_results\n\n# --- Class Definition 2: The Interactive Chatbot Class (Unchanged) ---\nclass InteractiveChatbot:\n    \"\"\"Handles the full RAG pipeline for interactive chat.\"\"\"\n    def __init__(self, model_path: str, vector_db_path: str):\n        print(f\"Loading QUANTIZED fine-tuned model from: {model_path}...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\"\n        )\n        \n        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        print(\"Connecting to existing vector database...\")\n        self.vector_db = PolicyVectorDB(vector_db_path)\n        \n        print(\"\\nInteractive Quantized Chatbot ready!\")\n\n    def retrieve_context(self, query: str, top_k: int = 3) -> list:\n        retrieved_results = self.vector_db.search(query, top_k=top_k)\n        RELEVANCE_THRESHOLD = 0.1\n        return [r for r in retrieved_results if r.get('relevance_score', 0) >= RELEVANCE_THRESHOLD]\n    \n    def format_prompt(self, query: str, context_results: list) -> str:\n        context_text = \"\"\n        if context_results:\n            for i, result in enumerate(context_results, 1):\n                metadata = result.get(\"metadata\", {})\n                source_info = f\"[Section: {metadata.get('section', 'N/A')}, Clause: {metadata.get('clause', 'N/A')}]\"\n                context_text += f\"Source {i}: {source_info}\\nDetails: {result.get('text', '')}\\n\\n\"\n        else:\n            context_text = \"No specific policy information was found for this question.\"\n            \n        return f\"\"\"<s>[INST] You are a helpful assistant for NEEPCO's Delegation of Power (DOP) policies. Use only the provided policy information to answer questions accurately and completely. Ensure you state the full rule, including all required actions, conditions, and approvals mentioned in the policy. If the provided information is insufficient, state that you cannot answer based on the given policies.\n\nPolicy Information:\n{context_text.strip()}\n\nQuestion: {query} [/INST]\n\nAnswer: Based on the provided policy information,\"\"\"\n\n    def chat(self, query: str):\n        context_results = self.retrieve_context(query)\n        \n        if not context_results:\n            print(\"\\nAnswer: I apologize, but I cannot find relevant information in the provided NEEPCO policy documents to answer your question.\")\n            return\n\n        prompt = self.format_prompt(query, context_results)\n        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        print(\"\\nAnswer: \", end=\"\")\n        _ = self.model.generate(\n            **inputs,\n            streamer=streamer,\n            max_new_tokens=512,\n            do_sample=True,\n            temperature=0.1,\n            top_p=0.9,\n            pad_token_id=self.tokenizer.eos_token_id\n        )\n        print(\"\\n\\n---\")\n        print(\"Sources Used:\")\n        for i, r in enumerate(context_results, 1):\n            meta = r['metadata']\n            print(f\"{i}. [Relevance: {r['relevance_score']:.2f}] Section {meta.get('section', 'N/A')}, Clause {meta.get('clause', 'N/A')}\")\n        print(\"-\" * 50)\n\n# --- Main execution block ---\n# --- Main execution block (SIMPLE, ONE-SHOT VERSION) ---\ndef start_chat():\n    # Check if required files/folders exist\n    if not os.path.exists(VECTOR_DB_PATH) or not os.path.exists(MODEL_PATH):\n        print(\"Error: Database or Quantized Model not found.\")\n        print(f\"Please ensure your vector DB folder exists at '{VECTOR_DB_PATH}'.\")\n        print(f\"And ensure the quantized model exists at '{MODEL_PATH}'.\")\n        return\n\n    # Initialize the chatbot once\n    chatbot = InteractiveChatbot(model_path=MODEL_PATH, vector_db_path=VECTOR_DB_PATH)\n\n    # --- ASK YOUR QUESTION HERE ---\n    # Change the text in the quotes below and re-run the cell to ask a new question.\n    user_query = \"What is the approval limit for a CGM for purchasing steel via open tender?\"\n    \n    print(f\"\\n\\nProcessing your question: '{user_query}'\")\n    if user_query:\n        chatbot.chat(user_query)\n    else:\n        print(\"Please write a question in the `user_query` variable.\")\n\n\n# Start the process\nif __name__ == \"__main__\":\n    start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:20:57.305671Z","iopub.execute_input":"2025-07-26T10:20:57.306221Z","iopub.status.idle":"2025-07-26T10:20:57.314472Z","shell.execute_reply.started":"2025-07-26T10:20:57.306192Z","shell.execute_reply":"2025-07-26T10:20:57.313653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python rag_chatbot_quantized.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:20:57.787620Z","iopub.execute_input":"2025-07-26T10:20:57.787850Z","iopub.status.idle":"2025-07-26T10:21:16.743383Z","shell.execute_reply.started":"2025-07-26T10:20:57.787833Z","shell.execute_reply":"2025-07-26T10:21:16.742595Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom IPython.display import FileLink, display\n\n# --- 1. Define the paths to your assets ---\n# The directory of your final, quantized model\nquantized_model_dir = \"./quantized_fine_tuned_tinylama_dop\"\n\n# The directory containing your ChromaDB vector database\nvector_db_dir = \"./policy_vector_db\"\n\n# The original dataset file that contains the processed chunks\n# Note: Adjust this path if it's different in your environment\nsource_dataset_file = \"/kaggle/input/dop-dataset/Dataset/combined_dataset.jsonl\"\n\n# The name for the final zip file\noutput_zip_name = \"dop_chatbot_deployment_package\"\n\n# A temporary directory to gather all files before zipping\nstaging_dir = \"./for_zipping\"\n\n\n# --- 2. Create a clean staging directory ---\nprint(f\"Creating temporary staging directory: {staging_dir}\")\nif os.path.exists(staging_dir):\n    shutil.rmtree(staging_dir)\nos.makedirs(staging_dir)\n\n\n# --- 3. Copy all assets to the staging directory ---\nprint(\"Copying assets...\")\n\n# Copy the quantized model\nif os.path.exists(quantized_model_dir):\n    print(f\"- Copying model from {quantized_model_dir}\")\n    shutil.copytree(quantized_model_dir, os.path.join(staging_dir, \"quantized_model\"))\nelse:\n    print(f\"WARNING: Model directory not found at {quantized_model_dir}\")\n\n# Copy the vector database\nif os.path.exists(vector_db_dir):\n    print(f\"- Copying vector database from {vector_db_dir}\")\n    shutil.copytree(vector_db_dir, os.path.join(staging_dir, \"vector_database\"))\nelse:\n    print(f\"WARNING: Vector DB directory not found at {vector_db_dir}\")\n\n# Copy the source dataset\nif os.path.exists(source_dataset_file):\n    print(f\"- Copying source dataset from {source_dataset_file}\")\n    shutil.copy(source_dataset_file, staging_dir)\nelse:\n    print(f\"WARNING: Source dataset not found at {source_dataset_file}\")\n\nprint(\"\\nAssets copied successfully.\")\n\n\n# --- 4. Create the zip archive ---\nprint(f\"\\nCreating zip file: '{output_zip_name}.zip'...\")\nshutil.make_archive(output_zip_name, 'zip', staging_dir)\nprint(\"Zip file created successfully.\")\n\n\n# --- 5. Clean up the temporary staging directory ---\nprint(\"Cleaning up temporary directory...\")\nshutil.rmtree(staging_dir)\nprint(\"Cleanup complete.\")\n\n\n# --- 6. Generate and display the download link ---\nprint(\"\\n-------------------------------------------\")\nprint(\"Your download is ready!\")\ndisplay(FileLink(f'{output_zip_name}.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:45:12.442983Z","iopub.execute_input":"2025-07-27T16:45:12.443776Z","iopub.status.idle":"2025-07-27T16:45:12.478752Z","shell.execute_reply.started":"2025-07-27T16:45:12.443748Z","shell.execute_reply":"2025-07-27T16:45:12.478041Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport subprocess\n\nprint(\"--- Starting GGUF conversion using llama.cpp (Addressing requirements.txt error) ---\")\n\n# Define base paths and file names\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\nLLAMA_CPP_REPO_NAME = \"llama.cpp\"\nLLAMA_CPP_DIR = os.path.join(KAGGLE_WORKING_DIR, LLAMA_CPP_REPO_NAME)\nLLAMA_CPP_BUILD_DIR = os.path.join(LLAMA_CPP_DIR, \"build\")\nLLAMA_CPP_BIN_DIR = os.path.join(LLAMA_CPP_BUILD_DIR, \"bin\")\n\n# Updated conversion script path (confirmed from your ls -F output)\nACTIVE_CONVERT_SCRIPT_NAME = \"convert_hf_to_gguf.py\"\nACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP = os.path.join(LLAMA_CPP_DIR, ACTIVE_CONVERT_SCRIPT_NAME)\n\n# Model and output paths\nsource_model_path = os.path.join(KAGGLE_WORKING_DIR, \"merged_fine_tuned_tinylama_dop\")\noutput_gguf_fp32_path = os.path.join(KAGGLE_WORKING_DIR, \"tinyllama_dop_fp32.gguf\")\nfinal_quantized_output_gguf_path = os.path.join(KAGGLE_WORKING_DIR, \"tinyllama_dop_q4_k_m.gguf\")\nquantization_method = \"Q4_K_M\"\n\n# --- Step 0: Clean previous attempts ---\nprint(f\"\\n0. Cleaning up previous '{LLAMA_CPP_REPO_NAME}' clone and old GGUF files (from {KAGGLE_WORKING_DIR})...\")\nos.chdir(KAGGLE_WORKING_DIR) # Ensure we are in the base working directory before cleaning\nif os.path.exists(LLAMA_CPP_DIR):\n    shutil.rmtree(LLAMA_CPP_DIR)\nif os.path.exists(output_gguf_fp32_path):\n    os.remove(output_gguf_fp32_path)\nif os.path.exists(final_quantized_output_gguf_path):\n    os.remove(final_quantized_output_gguf_path)\nprint(\"Cleanup complete.\")\n\n# --- Step 1: Clone llama.cpp ---\nprint(f\"\\n1. Cloning '{LLAMA_CPP_REPO_NAME}' repository...\")\ntry:\n    !git clone https://github.com/ggerganov/{LLAMA_CPP_REPO_NAME}.git\n    print(f\"'{LLAMA_CPP_REPO_NAME}' cloned into: {LLAMA_CPP_DIR}\")\nexcept Exception as e:\n    print(f\"ERROR: Git clone failed: {e}. Please check your internet connection or Kaggle environment.\")\n    exit()\n\n# --- DIAGNOSTIC: Verify clone contents ---\nprint(f\"\\nDIAGNOSTIC: Listing contents of '{LLAMA_CPP_DIR}':\")\n!ls -F {LLAMA_CPP_DIR}\nprint(f\"\\nDIAGNOSTIC: Checking for '{ACTIVE_CONVERT_SCRIPT_NAME}' at {ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}: {os.path.exists(ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP)}\")\n\n\n# --- Step 2: Install Python requirements for llama.cpp (from its root requirements.txt and manually) ---\nprint(f\"\\n2. Installing Python requirements for '{LLAMA_CPP_REPO_NAME}'...\")\n\n# Attempt to install from llama.cpp's main requirements.txt\nllama_cpp_main_requirements = os.path.join(LLAMA_CPP_DIR, \"requirements.txt\")\nif os.path.exists(llama_cpp_main_requirements):\n    print(f\"Installing from main llama.cpp requirements.txt: {llama_cpp_main_requirements}\")\n    !pip install -r {llama_cpp_main_requirements}\nelse:\n    print(f\"WARNING: Main llama.cpp requirements.txt not found at {llama_cpp_main_requirements}. Proceeding with manual installs.\")\n\n# Ensure core dependencies for conversion are installed explicitly (especially for conversion scripts)\nprint(\"Ensuring core conversion dependencies (transformers, torch, sentencepiece, accelerate, huggingface_hub) are installed explicitly...\")\n!pip install transformers==4.52.4 torch sentencepiece accelerate huggingface_hub==0.30.0\n\n# --- Step 3: Build llama.cpp using CMake ---\nprint(f\"\\n3. Building '{LLAMA_CPP_REPO_NAME}' using CMake...\")\nos.makedirs(LLAMA_CPP_BUILD_DIR, exist_ok=True)\ncurrent_dir_before_build = os.getcwd() # Save /kaggle/working\nos.chdir(LLAMA_CPP_BUILD_DIR) # Change to /kaggle/working/llama.cpp/build\n\ntry:\n    !cmake {LLAMA_CPP_DIR} # Configure CMake from llama.cpp root\n    !cmake --build . --config Release # Build the project\n    print(\"CMake build successful.\")\nexcept Exception as e:\n    print(f\"ERROR: CMake build failed: {e}. Check build logs above for details.\")\n    os.chdir(current_dir_before_build) # Change back\n    exit() # Exit if build fails\n\nos.chdir(current_dir_before_build) # Change back to /kaggle/working\nprint(f\"'{LLAMA_CPP_REPO_NAME}' built. Binaries should be in: {LLAMA_CPP_BIN_DIR}\")\n\n\n# --- Step 4: Convert your Hugging Face model to GGUF (float32) ---\n# IMPORTANT: Temporarily change directory to llama.cpp root BEFORE running the script\n# as it might expect its own requirements or helper files relative to its location.\nprint(f\"\\n4. Converting Hugging Face model from '{source_model_path}' to GGUF (float32) using '{ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}'...\")\nif not os.path.exists(ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP):\n    print(f\"CRITICAL ERROR: Conversion script not found at {ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}. Aborting.\")\n    exit()\n\ncurrent_dir_before_convert = os.getcwd() # Save /kaggle/working\nos.chdir(LLAMA_CPP_DIR) # Change to /kaggle/working/llama.cpp\n\ntry:\n    # Pass absolute paths for source and output as the script's current working directory has changed.\n    # The script name is now relative to the current directory (llama.cpp root).\n    !python {ACTIVE_CONVERT_SCRIPT_NAME} {source_model_path} --outfile {output_gguf_fp32_path} --outtype f32\n    print(\"Conversion script executed.\")\nexcept Exception as e:\n    print(f\"ERROR: Conversion script execution failed: {e}. Review the traceback above.\")\n    os.chdir(current_dir_before_convert) # Change back\n    exit() # Exit if conversion fails\n\nos.chdir(current_dir_before_convert) # Change back to /kaggle/working\n\n\n# --- Step 5: Quantize the GGUF model to a smaller, CPU-optimized format (e.g., Q4_K_M) ---\nprint(f\"\\n5. Quantizing GGUF model to {quantization_method}...\")\nif not os.path.exists(os.path.join(LLAMA_CPP_BIN_DIR, 'quantize')):\n    print(f\"CRITICAL ERROR: 'quantize' executable not found at {os.path.join(LLAMA_CPP_BIN_DIR, 'quantize')}. CMake build might have failed.\")\n    exit()\nelse:\n    !{os.path.join(LLAMA_CPP_BIN_DIR, 'quantize')} {output_gguf_fp32_path} {final_quantized_output_gguf_path} {quantization_method}\n\n# --- Step 6: Final Verification and Instructions ---\nprint(\"\\n--- GGUF Conversion (via llama.cpp) Process Complete ---\")\nprint(f\"Your final quantized GGUF model is located at: {os.path.abspath(final_quantized_output_gguf_path)}\")\nprint(f\"You can find the tokenizer files from your original model at: {os.path.abspath(source_model_path)}\")\nprint(\"\\n--- If the GGUF file was created, proceed to upload it and tokenizer files to Hugging Face Hub. ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport subprocess\n\nprint(\"--- Starting GGUF conversion using llama.cpp (Debugging 'quantize' executable build) ---\")\n\n# Define base paths and file names\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\nLLAMA_CPP_REPO_NAME = \"llama.cpp\"\nLLAMA_CPP_DIR = os.path.join(KAGGLE_WORKING_DIR, LLAMA_CPP_REPO_NAME)\nLLAMA_CPP_BUILD_DIR = os.path.join(LLAMA_CPP_DIR, \"build\")\nLLAMA_CPP_BIN_DIR = os.path.join(LLAMA_CPP_BUILD_DIR, \"bin\") # For 'quantize' executable\n\n# Updated conversion script path\nACTIVE_CONVERT_SCRIPT_NAME = \"convert_hf_to_gguf.py\"\nACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP = os.path.join(LLAMA_CPP_DIR, ACTIVE_CONVERT_SCRIPT_NAME)\n\n# Model and output paths\nsource_model_path = os.path.join(KAGGLE_WORKING_DIR, \"merged_fine_tuned_tinylama_dop\")\noutput_gguf_fp32_path = os.path.join(KAGGLE_WORKING_DIR, \"tinyllama_dop_fp32.gguf\")\nfinal_quantized_output_gguf_path = os.path.join(KAGGLE_WORKING_DIR, \"tinyllama_dop_q4_k_m.gguf\")\nquantization_method = \"Q4_K_M\"\n\n# --- Step 0: Clean previous attempts ---\nprint(f\"\\n0. Cleaning up previous '{LLAMA_CPP_REPO_NAME}' clone and old GGUF files (from {KAGGLE_WORKING_DIR})...\")\nos.chdir(KAGGLE_WORKING_DIR) # Ensure we are in the base working directory before cleaning\nif os.path.exists(LLAMA_CPP_DIR):\n    shutil.rmtree(LLAMA_CPP_DIR)\nif os.path.exists(output_gguf_fp32_path):\n    os.remove(output_gguf_fp32_path)\nif os.path.exists(final_quantized_output_gguf_path):\n    os.remove(final_quantized_output_gguf_path)\nprint(\"Cleanup complete.\")\n\n# --- Step 1: Clone llama.cpp ---\nprint(f\"\\n1. Cloning '{LLAMA_CPP_REPO_NAME}' repository...\")\ntry:\n    !git clone https://github.com/ggerganov/{LLAMA_CPP_REPO_NAME}.git\n    print(f\"'{LLAMA_CPP_REPO_NAME}' cloned into: {LLAMA_CPP_DIR}\")\nexcept Exception as e:\n    print(f\"ERROR: Git clone failed: {e}. Please check your internet connection or Kaggle environment.\")\n    exit()\n\n# --- DIAGNOSTIC: Verify clone contents ---\nprint(f\"\\nDIAGNOSTIC: Listing contents of '{LLAMA_CPP_DIR}' after cloning:\")\n!ls -F {LLAMA_CPP_DIR}\nprint(f\"\\nDIAGNOSTIC: Checking for '{ACTIVE_CONVERT_SCRIPT_NAME}' at {ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}: {os.path.exists(ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP)}\")\n\n\n# --- Step 2: Install Python requirements for llama.cpp (from its root requirements.txt and manually) ---\nprint(f\"\\n2. Installing Python requirements for '{LLAMA_CPP_REPO_NAME}'...\")\nllama_cpp_main_requirements = os.path.join(LLAMA_CPP_DIR, \"requirements.txt\")\nif os.path.exists(llama_cpp_main_requirements):\n    print(f\"Installing from main llama.cpp requirements.txt: {llama_cpp_main_requirements}\")\n    !pip install -r {llama_cpp_main_requirements}\nelse:\n    print(f\"WARNING: Main llama.cpp requirements.txt not found at {llama_cpp_main_requirements}. Proceeding with manual installs.\")\n\nprint(\"Ensuring core conversion dependencies (transformers, torch, sentencepiece, accelerate, huggingface_hub) are installed explicitly...\")\n!pip install transformers==4.52.4 torch sentencepiece accelerate huggingface_hub==0.30.0\n\n# --- Step 3: Build llama.cpp using CMake (modern build process) ---\nprint(f\"\\n3. Building '{LLAMA_CPP_REPO_NAME}' using CMake (VERBOSE)...\")\nos.makedirs(LLAMA_CPP_BUILD_DIR, exist_ok=True)\ncurrent_dir_before_build = os.getcwd() # Save /kaggle/working\nos.chdir(LLAMA_CPP_BUILD_DIR) # Change to /kaggle/working/llama.cpp/build\n\ntry:\n    print(\"Running CMake configuration (verbose)...\")\n    !cmake {LLAMA_CPP_DIR}\n    print(\"Running CMake build (verbose)...\")\n    # Add VERBOSE=1 to see all compilation details, which might reveal why 'quantize' fails.\n    !cmake --build . --config Release -- VERBOSE=1\n    print(\"CMake build process completed (check logs above for any specific warnings/errors for 'quantize').\")\nexcept Exception as e:\n    print(f\"ERROR: CMake build failed: {e}. Review the detailed build logs above.\")\n    os.chdir(current_dir_before_build) # Change back\n    exit() # Exit if build fails\n\nos.chdir(current_dir_before_build) # Change back to /kaggle/working\n\nprint(f\"'{LLAMA_CPP_REPO_NAME}' built. Binaries should be in: {LLAMA_CPP_BIN_DIR}\")\n\n# --- DIAGNOSTIC: List contents of the build/bin directory AND check quantize explicitly ---\nprint(f\"\\nDIAGNOSTIC: Listing contents of '{LLAMA_CPP_BIN_DIR}':\")\n!ls -F {LLAMA_CPP_BIN_DIR}\nprint(\"-\" * 50)\nquantize_executable_path_check = os.path.join(LLAMA_CPP_BIN_DIR, 'quantize')\n# Check both existence and executability\nif os.path.exists(quantize_executable_path_check) and os.access(quantize_executable_path_check, os.X_OK):\n    print(f\"DIAGNOSTIC: 'quantize' executable found and is executable at {quantize_executable_path_check}\")\nelse:\n    print(f\"DIAGNOSTIC: WARNING: 'quantize' executable NOT found or not executable at {quantize_executable_path_check}\")\n    print(\"This indicates a problem with the CMake build specifically for 'quantize'.\")\nprint(\"-\" * 50)\n\n\n# --- Step 4: Convert your Hugging Face model to GGUF (float32) ---\nprint(f\"\\n4. Converting Hugging Face model from '{source_model_path}' to GGUF (float32) using '{ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}'...\")\nif not os.path.exists(ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP):\n    print(f\"CRITICAL ERROR: Conversion script not found at {ACTIVE_CONVERT_SCRIPT_PATH_IN_LLAMA_CPP}. Aborting.\")\n    exit()\n\ncurrent_dir_before_convert = os.getcwd() # Save /kaggle/working\nos.chdir(LLAMA_CPP_DIR) # Change to /kaggle/working/llama.cpp\n\ntry:\n    !python {ACTIVE_CONVERT_SCRIPT_NAME} {source_model_path} --outfile {output_gguf_fp32_path} --outtype f32\n    print(\"Conversion script executed.\")\nexcept Exception as e:\n    print(f\"ERROR: Conversion script execution failed: {e}. Review the traceback above.\")\n    os.chdir(current_dir_before_convert) # Change back\n    exit()\n\nos.chdir(current_dir_before_convert) # Change back to /kaggle/working\n\n\n# --- Step 5: Quantize the GGUF model to a smaller, CPU-optimized format (e.g., Q4_K_M) ---\nprint(f\"\\n5. Quantizing GGUF model to {quantization_method}...\")\nquantize_executable_path = os.path.join(LLAMA_CPP_BIN_DIR, 'quantize')\nif not (os.path.exists(quantize_executable_path) and os.access(quantize_executable_path, os.X_OK)): # Check existence AND executability\n    print(f\"CRITICAL ERROR: 'quantize' executable not found or not executable at {quantize_executable_path}. CMake build might have had issues.\")\n    exit() # Exit if quantize not found\nelse:\n    print(f\"Executing quantize: {quantize_executable_path}\")\n    !{quantize_executable_path} {output_gguf_fp32_path} {final_quantized_output_gguf_path} {quantization_method}\n\n# --- Step 6: Final Verification and Instructions ---\nprint(\"\\n--- GGUF Conversion (via llama.cpp) Process Complete ---\")\nprint(f\"Your final quantized GGUF model is located at: {os.path.abspath(final_quantized_output_gguf_path)}\")\nprint(f\"You can find the tokenizer files from your original model at: {os.path.abspath(source_model_path)}\")\nprint(\"\\n--- If the GGUF file was created, proceed to upload it and tokenizer files to Hugging Face Hub. ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}